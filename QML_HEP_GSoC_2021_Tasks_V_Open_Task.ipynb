{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QML-HEP GSoC 2021 Tasks V Open Task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbNZLfmRPwHO"
      },
      "source": [
        "# Task V: Open Task Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd3ZFejT5v6L"
      },
      "source": [
        "Please comment on quantum computing or quantum machine learning. You can also comment on one quantum algorithm or one quantum software you are familiar with. You can also suggest methods you think are good and you would like to work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtXCRROz6b7_"
      },
      "source": [
        "## Comment on quantum computing or quantum machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St_Rt_roFNoD"
      },
      "source": [
        "**Note**: this is more of a personal opinion/comment rather than a scientific fact.\n",
        "\n",
        "When people talk about quantum machine learning (QML), they usually think about how a certain QML algorithm can bring something classical ML cannot do. Quantum speed-up, more hardware efficiency, and better performance are things that often come as obvious goals to be achieved. As of lately, many classical ML practitioners start questioning these things, a doubt that QML will ever be better than the classical ML arises. To be fair, the media seems to love hyping up quantum computing (and QML) to be bigger than it should be, increasing the risk of disappointment when it turns out not to be.\n",
        "\n",
        "In the era of NISQ, it is indeed tough to give a concrete result that QML is better. Variational quantum circuit or parametrized quantum circuit (PQC) (the building blocks for almost all QML algorithms) is invented to circumvent the NISQ devices limitation in the first place, which is to use classical computing to do the heavy lifting (optimizing the parameters). This is a good indicator that our algorithm design is greatly influenced by the readiness of the hardware we have right now. Considering a full-fledged quantum computer probably won't be realizable in the next several decades, does QML really worth it?\n",
        "\n",
        "One of the answers to this question that I think is true and very reasonable is Maria Schuld's answer. From several events having her as a speaker that I have attended, it summarizes to this:<br>\n",
        "Exploring the QML world is not always about quantum speed-up or how it can be better than the classical counterpart. Of course, it is exciting to see that happens, and at the end of the day, it is still the end/ultimate goal. But it is also important to think about how QML may bring many new and fresh ideas to the table. Understanding how things are done in QML can also be a source of new inspirations for classical ML, such as how tensor networks bring new ideas to classical ML[1]. I think this is the right mentality to have when talking about QML. While the potential is there, it still has a long way to go before beating up the classical ML in a real-world application. And who knows, maybe we will find some inspirations to make classical ML (or learning algorithm in general) better along the way. It is worth the try."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sufeW2P_51i0"
      },
      "source": [
        "## Quantum algorithm & software that I am familiar with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a3aEnFKAO_T"
      },
      "source": [
        "### Quantum Software"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvoMDnxg6U3G"
      },
      "source": [
        "I have been working in the field of quantum machine learning for about one year. My undergraduate thesis' topic is quantum machine learning (I will elaborate more about this in the next section). Quantum programming frameworks that I am most familiar with are PennyLane and Qiskit since I used both when I worked on my thesis. I just recently started using Cirq (and TFQ), and the transition was fairly easy because the style of Cirq and TFQ is very similar to TensorFlow, which I have used many times. I am also familiar with QuTiP. I used it a lot to visualize (image and video/GIF) quantum states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nmYxArh8TcT"
      },
      "source": [
        "Both PennyLane and TFQ have their own merits in prototyping a quantum machine learning model.\n",
        "\n",
        "PennyLane is a very flexible package since it can work with both TF/Keras and PyTorch. It also has better integrations with many simulators and real devices from other platforms/providers. The code performance (in terms of runtime) is sometimes not too good, probably because the developer team does not have much time to optimize it since it has too many integrations with other platforms/packages.\n",
        "\n",
        "On the other hand, TFQ is developed with only TF/Keras and Cirq in mind, so integrations with other platforms are not so well. But the performance in runtime seems to be better than PennyLane. Since the same company owns TF/Keras and TFQ, the integration between the two must be better and well-optimized than the PennyLane-Keras integration. One important issue of TFQ that I stumbled upon during working on the tasks is that the TFQ custom gate and controlled-gate module do not work seamlessly with the tensor-circuit converter. This makes designing circuits greatly harder than it should.\n",
        "\n",
        "One thing that PennyLane wins by a mile is in the educational/tutorial section. They have a [tutorial website](https://pennylane.ai/qml/demonstrations.html) which covered almost every corner in the QML world. This website makes prototyping with PennyLane very easy since many code templates are already given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weIXDbkXAUrm"
      },
      "source": [
        "### Quantum Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuRDCsk81DV"
      },
      "source": [
        "I understood and have some coding experience on many \"textbook\" quantum algorithms, e.g., VQE, QAOA, Grover's Algorithm, Quantum Fourier Transform, Quantum Phase Estimation, Shor's Algorithm, Bernstein-Vazirani Algorithm, Simon's Algorithm, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfHDvqya-BCo"
      },
      "source": [
        "Since last year, I have read many QML research papers. I also attended several online conferences such as QTML 2020 and Quantum 2020 by the Institute of Physics. Some quantum algorithms from research paper that I have implemented in code myself are as follow:\n",
        "\n",
        "1. The data re-uploading classifier from [“Data re-uploading for a universal quantum classifier”](https://quantum-journal.org/papers/q-2020-02-06-226/) (for thesis and Xanadu's QHack 2021 Open Hackathon).\n",
        "2. Layerwise learning from [“Layerwise learning for quantum neural networks”](https://link.springer.com/article/10.1007/s42484-020-00036-4) (for Xanadu's QHack 2021 Open Hackathon).\n",
        "3. VQE for excited states from [Variational Quantum Computation of Excited States](https://arxiv.org/abs/1805.08138) (for Xanadu's QHack 2021 QML Challenges).\n",
        "4. Quantum Graph Neural Networks from [Quantum Graph Neural Networks](https://arxiv.org/abs/1909.12264) (for the Qiskit Advocate Mentorship Program)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JheNVL-ys9EO"
      },
      "source": [
        "**Data Re-uploading Classifier (DRC)**<br>\n",
        "I would like to talk more about the DRC. It is a quantum algorithm that uses one-qubit (yes, one-qubit) PQC to theoretically classify data with any number of features and with any number of classes. In the original paper, the authors derived how this variational circuit is mathematically equivalent to the universal approximation theorem in classical neural networks. From my experience using this classifier, I can say that this algorithm works really well. DRC has been mentioned in several recent QML-related publications. One of them proposed a state-of-the-art VQE algorithm that incorporates ideas from the DRC algorithm[2].\n",
        "\n",
        "This is also the method that I would like to suggest. I will elaborate more in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfbCLSnR6C5d"
      },
      "source": [
        "## Methods Suggestions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BmjsW1yB5cf"
      },
      "source": [
        "I have always been amazed by the [data re-uploading classifier](https://quantum-journal.org/papers/q-2020-02-06-226/), both in performance (accuracy) and in resources (only need a minimal number of qubits). I have used it on two occasions, and both give successful results.\n",
        "\n",
        "1. In my thesis, I used it to classify the PCA-transformed MNIST dataset (2 classes) and got more than 99% of accuracy by just using 1 qubit. This is a significant improvement if compared to some previous related works[3-5]. It also performed well on multi-class classification. It got more than 95% of accuracy for 4-class classification just by using 2 qubits. The code can be accessed [here](https://github.com/eraraya-ricardo/quantum_image_classifier/blob/master/PennyLane/Data%20Reuploading%20Classifier/2_PCA_QFC%20(best).ipynb) and [here](https://github.com/eraraya-ricardo/quantum_image_classifier/blob/master/PennyLane/Data%20Reuploading%20Classifier/4_PCA_QFC_BinRep%20(best).ipynb).\n",
        "\n",
        "2. During Xanadu's QHack 2021 Open Hackathon, my team proposed to use it and layerwise learning from reference [2] to classify high-energy physics events in [SUSY dataset](https://archive.ics.uci.edu/ml/datasets/SUSY#). We obtained better AUC compared to the paper [Event Classification with Quantum Machine Learning in High-Energy Physics](https://arxiv.org/abs/2002.09935). We won second place in the event. The code can be accessed [here](https://github.com/eraraya-ricardo/qhack-2021-openproject).\n",
        "\n",
        "This classifier seems to work really well. I think it worth a try for both of the QML-HEP projects, but mainly the \"Quantum Neural Networks for High Energy Physics Analysis at the LHC\" project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ab-OIu2DXKm"
      },
      "source": [
        "# References\n",
        "\n",
        "1. [E. M. Stoudenmire and D. J. Schwab, “Supervised Learning with Quantum-Inspired Tensor Networks,” May 2016.](https://arxiv.org/abs/1605.05775)\n",
        "2. [A. Cervera-Lierta, J. S. Kottmann, and A. Aspuru-Guzik, “The Meta-Variational Quantum Eigensolver (Meta-VQE): Learning energy profiles of parameterized Hamiltonians for quantum simulation,” arXiv, Sep. 2020.](https://arxiv.org/abs/2009.13545)\n",
        "3. [E. Farhi and H. Neven, “Classification with Quantum Neural Networks on Near Term Processors,” pp. 1–21, 2018.](https://arxiv.org/abs/1802.06002)\n",
        "4. [A. Skolik, J. R. McClean, M. Mohseni, P. van der Smagt, and M. Leib, “Layerwise learning for quantum neural networks,” Quantum Mach. Intell., vol. 3, no. 1, p. 5, Jun. 2021.](https://link.springer.com/article/10.1007/s42484-020-00036-4)\n",
        "5. [S. Mardirosian, “Quantum-enhanced Supervised Learning with Variational Quantum Circuits,” Leiden University, 2019.](https://theses.liacs.nl/pdf/2018-2019-MardirosianSevak.pdf)"
      ]
    }
  ]
}